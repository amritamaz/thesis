% !TEX root = paper.tex

\begin{abstract}


\noindent Cameras are the defacto sensor.
The growing demand for real-time and low-power computer vision, coupled with trends towards high-efficiency heterogeneous systems, has given rise to a wide range of image processing acceleration techniques at the camera node and in the cloud.
In this paper, we characterize two novel camera systems that use acceleration techniques to push the extremes of energy and performance scaling, and explore the computation-communication tradeoffs in their design.
The first case study targets a camera system designed to detect and authenticate individual faces, running solely on energy harvested from RFID readers.
We design a multi-accelerator SoC design operating in the sub-mW range, and evaluate it with real-world workloads to show performance and energy efficiency improvements over a general purpose microprocessor.
The second camera system supports a 16-camera rig processing over 32 Gb/s of data to produce real-time 3D-360$^{\circ}$ virtual reality video.
We design a multi-FPGA processing pipeline that outperforms CPU and GPU configurations by up to 10$\times$ in computation time, producing panoramic stereo video directly from the camera rig at 30 frames per second.
We find that an early data reduction step, either before complex processing or offloading, is the most critical optimization for in-camera systems.
% This paper presents case studies on these camera system designs, holistically evaluates each system's computation--communication tradeoffs, and distills general lessons learned from engineering them.

\end{abstract}

%We explore the design space of hardware-software co-design for image processing to realize tightly-optimized in-camera processing pipelines.
% For continuous mobile face authentication,
% For virtual reality video assembly, we show how processing at the camera rig level can overcome bandwidth limitations and achieve real-time performance.
% We present a multi-FPGA accelerator design that can process 360$^{\circ}$ 4K stereoscopic video streams at 30 frames/second from a 16-camera rig.


% sensor  and are fast becoming the universal sensor
% Mobile camera systems face a tradeoff between in-camera processing and cloud offload.
% In-camera processing is limited in performance and battery life, but cloud offload presents time and energy costs that make real-time processing infeasible.
% In this paper, we evaluate the hardware-software design space of camera-centric image processing applications.
% In particular, we explore heterogeneous implementations of computational blocks in image processing pipelines, and evaluate how and where to implement these pipelines.
% We conduct two case studies: (i) continuous ultra-low-power face authentication from energy-harvesting cameras and (ii) 3D-360$\degree$ stereoscopic virtual reality video assembly from 16 HD cameras in real time.
% Both applications have extreme constraints on power or performance.
% %We explore the design space of hardware-software co-design for image processing to realize tightly-optimized in-camera processing pipelines.
% For continuous mobile face authentication, we evaluate a multi-accelerator SoC design and show 3-8 orders of magnitude improvement in performance and energy efficiency over a general purpose microprocessor.
% The accelerators are implemented down to layout on a 65nm TSMC process, and they fit in \textasciitilde 0.5$mm^2$ while operating in the sub-mW range.
% For virtual reality video assembly, we show how processing at the camera rig level can overcome bandwidth limitations and achieve real-time performance.
% We present a multi-FPGA accelerator design that can process 360$^{\circ}$ 4K stereoscopic video streams at 30 frames/second from a 16-camera rig.
