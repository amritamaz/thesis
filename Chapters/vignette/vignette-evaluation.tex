%!TEX root = ../paper.tex


\section{Evaluation}
\label{sec:vignette-eval}

We designed our evaluation to answer the following questions:

\begin{itemize}
 \item \textbf{Storage:} What storage and bandwidth savings does \name provide? How do tile configurations affect compression gains and quality?
 \item \textbf{Quality of Service:} How does \name's compression technique affect quality of service (QoS) of video services like video streaming (perceptual quality user study) or machine learning (speed, accuracy)? % Does perceptual compression hamper the quality of standard video services?
 % \item[\ding{205}] \textbf{User Study:} How do viewers respond to our perceptual compression implementation?
 \item \textbf{Compute Overhead:} What is the computational overhead of \name's compression algorithm and storage manager?
 \item \textbf{Data Center \& Mobile Cost:} How do Vignette's storage and network bandwidth savings impact video storage system and mobile viewing costs?
\end{itemize}

\subsection{Storage and Bandwidth Savings}
\label{subsec:storage}
To evaluate the storage and bandwidth benefits of \name, we applied \nameCompress to the corpus of videos described in Section~\ref{sec:vignette-methodology}.
We transcoded our video library at iso-bitrate in salient regions and decreased bitrate linearly with saliency to a minimum 10\% target bitrate in the lowest saliency tiles, as illustrated in Figure~\ref{fig:saliency-tiles-overview}.
In these experiments, we examine how our transcoding performs across a range of resolutions and workloads, as is expected in a video storage system.


\paragraph{Impact of Tiling on Compression and Quality: } We first examined the impact of tiling on compression benefits using a fixed saliency map.
% We used a linear mapping of saliency values to constant bitrate to encode saliency information in individual HEVC tiles.
We used an exhaustive tile configuration search and evaluated all tile sizes to identify an optimal number of tiles for each video.
We observed that, given a fixed saliency map, optimal tile configurations to maximize storage savings and quality varied based on entropy and video content.
Some videos benefited from many small tiles, while others performed best with fewer large tiles.

\tileVsSizeFigure

The smallest tile size we evaluated were 64 pixels in breadth, but most videos performed best with tiles having a breadth of 300--400 pixels.
As Figure~\ref{plot:tile-vs-size} shows, this experiment indicated that the optimal tile configuration for a video is content-dependent and can vary from four tiles to forty, and that tile configuration is an important component of tile-based compression.


\paragraph{Overall Compression, Bandwidth, Quality: }We next explored peak compression, bandwidth, and quality savings by applying \name to our video corpus and evaluating compression and quality savings.
We used the results of our exhaustive tile search to identify the best compression-quality configurations for each video.
Figure~\ref{fig:average-storage-bitrate-savings} shows aggregate storage savings, partitioned by  dataset.

\avgStorageBitrateSavings

Overall, we find that \nameCompress produces videos that are 1--15\% of the original size when maintaining the original bitrate in salient regions.
These compression savings include the fixed overhead of perceptual metadata, which is $<$100 B for all videos.
Datasets with higher video resolutions (Blender, VR-360) demonstrated the highest compression savings.
The vbench dataset, which is algorithmically chosen to have a wide variance in resolution and entropy, exhibits a commensurately large variance in storage reduction.
Of the videos with the lowest storage reduction, we find that each tends to have low entropy, large text, or other 2D graphics that are already efficiently encoded.


Table~\ref{table:dataset-quality} shows the average reduction in bitrate and resulting quality, measured in PSNR and EWPSNR.
Our results show that EWPSNR results are near-lossless for each benchmark dataset.
The PSNR values---which do not take the human visual processing system into account--- are lower, but still above the 35 dB threshold, and so remain acceptable for viewing.

\begin{table}[h]
  \caption{Average bitrate reduction and quality measurements for \nameCompress by dataset. For PSNR and EWPSNR, $>$ 30 dB is acceptable for viewing, 50 dB+ is lossless.}
  \label{table:dataset-quality}
  \centering
  % \resizebox{\linewidth}{!}{%
    \begin{tabular}{lccc}
      \toprule
       & Bitrate  & PSNR & Eye-weighted \\
      Benchmark & Reduction &   (dB) & PNSR (dB)  \\ \midrule
      vbench       &   85.6 \%     & 39       &   51                                    \\
      Netflix      & 98.6\phantom{ \%}       & 34        &  45                                 \\
      VR-360      & 98.8\phantom{ \%}        & 36        &  45                                \\
      Blender     & 98.2\phantom{ \%}        & 39        &  49               \\ \bottomrule
    \end{tabular}
  % }
\end{table}




Figure~\ref{fig:example-vid-saliency} highlights a \name video frame from the Netflix dataset, with an output PSNR of 36 dB and EWPSNR of 48 dB.
Overall, the results indicate that \nameCompress provides acceptable quality for its compression benefit.


\subsection{Quality of Service}
\label{subsec:qos}
To understand the impact of perceptual compression on common video system workloads, we evaluated quality of service (QoS) delivered by \name for two applications: entertainment streaming with a user study and evaluation of a video analytics application that performs object recognition.
These applications optimize for different QoS metrics: perceptual quality for entertainment video, and throughput and accuracy for object recognition.

\exampleVidSaliencyFigure

\paragraph{Perceptual Quality User Study:} We ran a user study to quantify viewer perception of our saliency-based compression.
The study presented users with two versions of the same video: one encoded with \hevc at 20 Mbps, the other with \nameCompress.
The \nameCompress videos were randomly chosen to be either 1 Mbps, 5 Mbps, 10 Mbps, or 20 Mbps.
% One video in the pair was encoded with \hevc and the other with \nameCompress.
The study asked users their preference between the matched pairs for 12 videos.
The bitrate of the \nameCompress video varied randomly across the questionnaire.
The goal was to discover if viewers prefer \nameCompress to \hevc, and, if so, if those preferences are more or less pronounced at different bitrate levels for \name.

The 12 videos included three videos from each dataset, selected to cover a range of entropy levels.
All videos had original bitrates above 20Mbps, except two from vbench.
Each video was encoded at a target bitrate (1Mbps, 5Mbps, 10Mbps, or 20Mbps), and the questionnaire randomly selected which bitrate to serve.
We distributed the questionnaire as a web survey and ensured videos played correctly in all browsers by losslessly re-encoding to \avc.

We recruited 35 naive participants aged 20--62 (51\% women, 49\% men) from a college campus to participate in the study.
Figure~\ref{plot:user-study} shows the results averaged across subjects and videos.
When \name videos are encoded at 1 Mbps in the most salient regions, 72\% users preferred the \hevc baseline.
However, for \name videos encoded at 5, 10, and 20 Mbps, users either could not tell the difference between \hevc and \name, or preferred \name videos 60\%, 79\%, and 81\% of the time, respectively.
This suggests that video systems can deliver \name-encoded videos at 50-75\% lower bitrate with little perceived impact.

\userStudyFigure



\paragraph{Object classification: } Video storage and processing systems often perform analytics and machine learning tasks on their video libraries at scale~\cite{poms2018scanner,shen2017deepvideo,zhang2017livevideoanalytics}.
To evaluate any performance degradation in latency or quality from using \nameCompress, we profile \name while running  YOLO~\cite{redmon2017yolo}, a popular fast object recognition algorithm.
We compare against baseline \hevc-encoded videos to evaluate if \name incurs any additional cost in a video processing setting.

\begin{table}[h]
  \centering
  \caption{\name Speedup and Accuracy Compared to \hevc Baseline on YOLO Object Recognition.}
  \label{table:yolo}
\begin{tabular}{ccc} \toprule
  Decode              & Total Speedup             &  Average    \\
  Speedup             & (Decode + YOLO)           & Accuracy \\ \midrule
  34.6\% $\pm$ 14.3\% & 2.6\% $\pm$ 2.2\% & 84\% $\pm$ 14\% \\ \bottomrule
\end{tabular}
\end{table}

Table~\ref{table:yolo} shows that using \name-compressed videos provides some speedup when decoding videos for object recognition, but this benefit is overshadowed by the cost of running YOLO.
Examining accuracy, we find that \name videos maintain 84\% accuracy on average, compared to the baseline \hevc videos.
% \todo{break out table by benchmark suite}
We find that accuracy on the YOLO task is lowest for the videos in the VR360 suite, and tends to correspond to the areas where the video is distorted from the equirectangular projection.
While saliency-compressed videos can provide slight benefits for video analytics latency, especially if video decoding is the system bottleneck, future work should investigate how to optimize saliency-based compression for video analytics.


\subsection{Compute Overhead}
\nameCompress bears the additional processing overhead of executing a neural network to generate or update saliency maps.
\nameStore can switch between an exhaustive or more computationally-efficient heuristic tile configuration search to uncover optimal tile configurations for a video.
We benchmarked the latency of the combined saliency and transcoding pipeline in two modes: exhaustive, which generates saliency maps per frame and exhaustively evaluates tiling, and heuristic, which uses the heuristic search algorithm to select a tile configuration within 0.25 dB of the best-PSNR choice (Section~\ref{subsec:search-algo}).

\computeTable

Table~\ref{table:compute} shows generating saliency maps in either mode dominates computation time for \name, and that our heuristic search is 33$\times$ faster than an exhaustive search.
This step, however, is only executed once per video and off the critical path for video streaming workloads.


\subsection{Analytical Model of \name Data Center and Mobile Costs}
\label{subsec:datacentermobile}
We use our evaluation results to model \name's system costs at scale for data center storage and end-user mobile power consumption. While these results are a first-order analysis, they suggest the potential benefit of deploying \name.

% \vspace{.25cm}
\paragraph{Data center compute, storage, and network costs. }
Given the high compute cost of \name, we evaluate the break-even point for systems that store and deliver video content.
We used Amazon Web Services (AWS) prices from July 2018 in the Northern California region to characterize costs.
We use a \texttt{c5.xlarge} instance's costs for compute, S3 for storage, and vary the number of videos transferred to the Internet as a proxy for video views.

\awsBreakevenFigure


We assume a video library of 1 million videos that are 10 MB each, encoded at 100 different resolution-bitrate settings (as in~\cite{huang2017sve,netflix2018dynamicopt}) to produce $\sim$500 TB of video data.
We estimate baseline compute cost to be a two-pass encoding for each video at $\$0.212$ / sec and \name's transcode computation to be $5\times$ a baseline transcode.
Larger companies likely use Reserved or Spot Instance offerings, which provide better value for years-long reservation slots or non-immediate jobs; they are 36\% and 73\% cheaper, respectively.
For storage, we estimate costs to be \$0.023 / GB on S3 and assume Vignette-compressed videos would be 10\% of the original videos (Section~\ref{subsec:storage}).
Transferring data out from S3 costs \$0.05 / GB; this cost is where \name achieves the majority of its savings.


% Cost(views) = Proccesing(videos) + Storage(videos) + Network(views)
% P(videos) = videos*encoding_options*transcode_time
% S(videos) = videos*video_size*encoding_options
% Network(views) = views*video_size

Figure~\ref{fig:aws-utilization} shows how different compute pricing models produce different savings at small numbers of video library views, but that \name becomes cost-effective at large video viewing rates.
For all compute pricing levels, a system would need to service $\sim$2 billion views across a million-video library before \name's compute overhead would be amortized across transmission and storage savings.
This number is easily reached by large video services; Facebook reported 8 billion daily views in 2016~\cite{fbviews}.

\powerFigure

% \vspace{.25cm}
\paragraph{Mobile Power Consumption.}
We explicitly designed \name to work with the \hevc{} standard so off-the-shelf software and hardware codecs could decompress \name videos.
\nameCompress's tiling strategy, however, makes video bitstream density highly non-uniform across the visual plane.
This results in inefficiency for hardware architectures that decode variably-sized tiles in parallel.
On the other hand, even such designs will achieve a higher overall power efficiency because of the reduced file sizes to decode and display.
To investigate whether \name videos can achieve power savings, we profiled power consumption on a Google Pixel 2 phone during video playback of \name videos and standard \hevc-encoded videos.


We measured battery capacity on a Google Pixel 2 running Android 8.1.0, kernel 4.4.88-g3acf2d53921d, playing videos on MX Player v1.9.24 with ARMv7 NEON instructions enabled.
When possible, MX Player used hardware acceleration to decode videos.\footnote{MX Player only supported decoding stereoscopic videos with the software decoder.}
We disabled nonessential display and button backlights, as well as any configurable sensors or location monitors, to minimize extraneous power consumption.
We logged battery statistics each minute using 3C Battery Monitor Widget v3.21.8.
We conducted three trials, playing the 93-file video library in a loop until battery charge dissipated from 100\% to 30\%, for our \hevc baseline and \name videos.

Figure~\ref{fig:power} shows our results.
We found that \name video enabled 1.6$\times$ longer video playback time with the same power consumption, or, $\sim$50\% better battery life while viewing a fixed number of videos.
While hardware decoder implementations are typically proprietary, these results indicate that perceptual compression has benefits for mobile viewers, as well as cloud video infrastructure.

\subsection{Discussion}

\paragraph{Using \name in Other Storage Systems.}
We crafted the policies, metadata extensions, and encoder design to make the ideas behind \nameStore{} compatible with pre-existing video storage or transcoding systems.
\name can easily be implemented for other codecs, like the upcoming \avone,
For instance, an Amazon MediaConvert instance with a storage layer in Amazon EBS and Glacier can easily use the policies and metadata in Section~\ref{sec:system} to implement \name.
We could further improve \name by building on other optimizations that work with off-the-shelf video standards.
For instance, \name's heuristic search algorithm could include power and performance information from open-source video transcoding ASICs~\cite{asicclouds, zhang2017racetosleep} to target more power-efficient tiling configurations.
VideoCoreCluster~\cite{liu2016greenvid} demonstrated energy-efficient adaptive bitrate streaming in real-time using a cluster of low-cost transcoding ASICs, which \name could leverage for better server transcoding performance.

\paragraph{Other video system optimizations.}
Using Fouladi~\etal's parallel cloud transcoding could also improve \name's transcode latency, and \name's saliency-based tiling could integrate with their codesigned network transport protocol and video codec to better tune streaming quality~\cite{fouladi2018salsify, fouladi2017excamera}.
At the physical storage layer, Jevdjic \etal's approximate video storage framework, which maps video streams to different layers of error correction, could be coupled with \name's saliency mapping for more aggressive approximation of non-salient video regions~\cite{jevdjic2017approxvid}.
Integrating \name with these systems could further improve power efficiency during playback, transcoding latency, or archival video storage durability.
