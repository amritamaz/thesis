%!TEX root = ../paper.tex

\section{Background: Perceptual Compression Using Saliency Maps}

\label{sec:saliency}
Saliency is a widely-utilized measure of the perceptual importance of visual information.
Saliency data encodes the perceptual importance of information in a video, such as foreground and background or primary and secondary objects.
Video codecs already use some perceptual information, like motion and luminance, to improve compression performance~\cite{hevc}, but new modes of video viewing (such as with a VR headset) introduce the opportunity to integrate richer cues from the human visual system~\cite{lee2012perceptualcodingsurvey}.
In this paper, we use saliency as an example of one such perceptual cue to demonstrate the potential of perceptual compression.
This section provides background on saliency, compares methods for generating and encoding saliency information, and introduces the machine learning technique \name uses to gather perceptual information about video data.
We also describe \emph{tiles}, the codec feature we use to compress videos with saliency information.

\subsection{Salience Maps and Detection Algorithms} Saliency-detection algorithms highlight visually significant regions or objects in an image.
A saliency map captures visual attention in the form of a heatmap, where the map's values correspond to the salience of pixels in the input.
In this paper, we visualize saliency maps as grayscale video frames or heatmaps for clarity.
% \name, however, uses a more efficient technique for saliency map compression, described in~\ref{sec:tiled-compression}.

In the past, saliency information was hard to generate accurately without detailed per-video information, such as hand annotation or detailed eye gaze logs.
Moreover, the low latency and poor spatial resolution of eye-tracking devices prevented effective deployment of eye-tracker-based saliency prediction~\cite{bulling}.
VR headsets, however, are natural environments for the efficient deployment of eye tracking, and they have motivated improvements in the performance and accuracy of eye trackers~\cite{Whitmire:2016:ESC:2971763.2971771}.
Recent work combining these improved eye tracker-generated fixation maps with deep learning has improved the accuracy of saliency prediction algorithms, especially for natural images and video~\cite{bylinskii2016saliency}.

\subsection{Systems Support for Perceptual Video Compression} Prior work has investigated many techniques for including saliency information in video compression, reducing bitrate at iso-quality by 20-60\%.
However, these techniques required significant changes to video codecs, i.e., maintaining full-frame saliency maps to use as additional input~\cite{8117038}, computing saliency models on-the-fly at high computational cost~\cite{5223506}, or solving complex optimization problems to allocate video bits~\cite{li2011visual}, as the quality of saliency map generation was the limiting factor to deploying perceptual compression.
% \todo{explain that we don't actually want to plug perceptual compression into the video encoer}
Recent interest in applying machine learning techniques to problems in visual comprehension resulted in accurate saliency prediction models that effectively mimic the human visual system~\cite{bylinskii2016saliency}.
Moreover, interest from VR developers in deploying fast and accurate eye tracking for improved VR experiences further improved the accuracy of saliency maps with high quality fixation data, leading to a virtuous cycle of saliency map prediction and improvement~\cite{Whitmire:2016:ESC:2971763.2971771}.
The final challenge in closing the gap for deploying perceptual video compression is to design storage systems that manage and support perceptual information.

\subsection{Tiled Video Encoding} \name uses tiles to implement perceptual compression.
Tiling a video divides a single video stream into independent regions that are encoded as separate decodable streams \cite{6547985}.
Encoders can code tiles at separate qualities or bitrates, and decoders can decode tiles in parallel.
Tiles are simple to express using standard encoding libraries, like \texttt{FFmpeg}~\cite{ffmpeg} and are supported in many video codecs.
% \name uses the spatial video tile implementation provided by \hevc~\cite{6547985}.
Restricting our implementation to native tiling features introduces some loss of detail compared to designing a custom encoder.
Standard encoders only support rectangular tiles, and cannot leverage motion across tiles during encoding process.
Using only native features, however, guarantees that our compression scheme is compatible with any modern codec that implements tiling, like \hevc or \avone~\cite{avone}.
As video standards and codec efficiency improve, using general codec features to perform encoding and manage storage ensures that perceptual information remains useful.
\begin{comment}
  \section{Automatically Generating Saliency Maps}
  \name generates saliency maps for existing video datasets using \mbox{MLNet~\cite{mlnet2016}}, a convolutional neural network that extracts saliency features and uses learned priors to encode those features into a single saliency model.

  A saliency predicting algorithm identifies important objects or regions. A saliency map then captures that information in the form of a heatmap overlaid on top of video frames.

  Saliency is closely related to eye fixation, or where a user's eyes dwell in a scene.
  At a semantic level, eye fixation does not necessarily correlate with visual salience, such as when a task-focused user may ignore a spatial area that would otherwise be visually interesting.

  In prior work, the low latency and poor spatial resolution of eye-tracking devices prevented effective deployment of eye-fixation-based saliency prediction~\cite{bulling}.
  Once generated, this saliency information
  As videos increase in resolution and viewing media diversity, the region-of-interest areas in a video scale inversely, further increasing the potential gains from integrating saliency into video compression~\cite{sitzmann2018saliency, lee2012perceptualcodingsurvey}.
  \brandon{It's ROI/diversity that is motivating work in saliency?  If that's not the important part, why not just ``One such cue is \textit{saliency}, which... ''  Or was ``viewing media diversity'' supposed to capture new modes of viewing / VR headsets?  If so, it wasn't clear to me.}
  However, saliency information does not necessarily correlate with other motion or frequency information traditionally used by modern video codecs, and, as such, is an underutilized source of compression information.\brandon{Is it really underutilized because of the correlation, as stated?  Ideally you'd link the ``new modes of viewing'' from the first sentence to the increased applicability of saliency.  But maybe that's because I misunderstood the second sentence.}

  Video consumers primarily consume video on mobile devices with cameras, gyroscopes, and stabilization sensors.

  future media systems, e.g., VR headsets with sensor-rich mobile devices with large immersive displays and high-accuracy eye trackers, provide new opportunities to leverage context-aware video viewing trends and characteristics for video compression.
  Entertainment content providers like Netflix already leverage information about mobile viewing habits to optimize video quality for display and device characteristics~\cite{netflix2015encoding}.
  Most videos are stored for the purposes of viewing by humans or for human analytics, both which are concerned with the perceptual information in the video, not necessarily the raw bit representation of each pixel.

  \section{Saliency from Eye Fixation Maps}
  \brandon{This also feels like related work -- do I need to know it to survive sections 3-7?  Maybe just because I haven't read the latest introduction in detail yet.}
  are a core codec feature, providing functionality to

  , and present low overhead compared to similar implementations with older video coding features, like slices.
  Predicting salience influences many fundamental tasks in human and computer vision, such as object recognition, image/video compression, summarization, and content-aware resizing~\cite{lee2012perceptualcodingsurvey}.
  Early techniques used hand-coded features to capture saliency~\cite{itti1998model}, but recent deep learning-based methods provide near-perfect saliency prediction~\cite{mlnet2016, huang2015salicon, 7298731}.

  , and we found it generates acceptable saliency maps for our video compression.

  \brandon{Unfortunately the images are a little small to demonstrate how the visual importance is captured.}

  but we also describe in~\ref{} a method for compressing saliency maps into a low-overhead bitstring during compression.\brandon{If \name always does this, maybe change to read ``However, \name uses a more efficient technique for saliency map compression, which we describe in section X.'' and leave the bitstring details for later.}

\end{comment}
