\section{Video Energy Estimates with the {mVDOprof} Framework}
\label{sec:vdoprof}

While new video decode architectures like \nameArch can improve energy efficiency for video playback in constrained settings, evaluating this improvement across a range of different video encoding parameters, resolutions, and bitrates is challenging.
Moreover, without access to detailed specifications and hardware designs, comparing performance across new and existing video decode architectures presents challenges.
To resolve these issues, we design \nameArchprof, our profiling and simulation framework for estimating energy consumption for perceptually compressed videos.
\nameArchprof targets mobile video decoder architectures, abstracting codec-specific complexity so creators of perceptual video compression software can evaluate the energy efficiency of their encoded videos.
We use \nameArchprof in this paper to characterize energy-efficiency of videos using \nameArch's simulated decoder architecture. \nameArchprof can also characterize video energy on other existing architectures.
% \nameArchprof helps VR developers optimizing video for mobile devices to tune their perceptual compression for optimal energy consumption.


\subsection{\nameArchprof Overview}
\nameArchprof's characterizes video decode energy and latency, taking into account new perceptual compression methods to optimize energy consumption.
Current approaches to these characterizations involve running exhaustive experiments on real-world hardware~\cite{evr19isca} or hand-tuning a simulator based on research prototypes and validating against full-system traces~\cite{zhang2017race}.
\nameArchprof automates and codifies this process, decoupling the relationship between hardware decoders, input videos, and playback constraints.
This enables more efficient characterization of video decode performance across more decode devices, yielding more energy-efficient VR video applications.
Figure~\ref{fig:vdo-prof} shows a high-level \nameArchprof architecture.

\vdoprofOverview

\subsection{Hardware Decoder Profiles}
\label{subsec:profiles}

Given a new hardware decoder, \nameArchprof first automates profiling on large video workloads to construct a model of decode performance, the \emph{hardware decoder profile}.
This profile collects information about how a decoder performs on video benchmarks in terms of power and latency to help characterize its performance on new videos.
It contains configuration and statistics files for \nameArchprof and can be distributed or shared to facilitate analyzing video decode performance without access to the physical accelerator.

\nameArchprof also provides an interface to construct a model from provided parameters for research prototypes or simulated chips.
Hardware vendors could provide \nameArchprof decoder profiles for validation, or consumer VR video application developers could build profiles for each hardware decoder their application supports.

To generate a hardware decoder profile from an existing design, the developer would run \nameArchprof on the device with a power-monitoring interface enabled.
For instance, to characterize the hardware decoder provided with the NVIDIA Jetson TX2 board, \nameArchprof would use the on-board power monitor, to capture power consumption and latency on the video benchmarks.
\nameArchprof would also instrument the characterization video datasets using {FFmpeg}\cite{ffmpeg} and extract video statistics to profile decode events from existing data.
\nameArchprof would log accelerator performance on videos varying in the complexity and size of video tiles, to assess parallelism and energy efficiency.
\nameArchprof would then link this information to characteristics of the test videos, like number of CTUs and CTU complexity in the tile.
From these logs, \nameArchprof would construct a hardware decoder profile.
A developer could then use this extrapolated hardware decoder profile to estimate latency and power consumption of new videos without requiring access to the physical accelerator.

\nameArchprof also supports generating profiles on integrated or next-generation hardware where capturing power numbers \emph{in situ} is challenging.
For instance, to profile the hardware decoder on a Google Pixel 2 phone, the battery status monitor could be polled as a proxy for power consumption.
For devices without a physical interface, e.g., next-generation decode accelerators or experimental prototypes, \nameArchprof could generate a decoder profile from a given specification or simulator that provides per-video power and latency numbers.

\subsection{Video Encoding Constraints}
\nameArchprof provides optimal encoding parameters to support energy efficiency for a VR video application.
Some applications pre-determine or constrain the number of tiles used (to optimize video quality, or for integration with other performance optimizations) or mandate a specific codec for compatibility.
VR video applications can specify the type of perceptual compression, such as a foveation pattern, to be explored in the simulation process.
\nameArchprof ingests these constraints and use it to guide energy consumption estimates.

\subsection{Device Playback Constraints}
\nameArchprof can optimize for a specific display device target resolution, frame rate, or potential battery life.
For instance, a developer seeking to optimize video encoding to ensure the entirety of the video plays within a device's battery life could specify it via playback constraints.
If playback constraints are not set, \nameArchprof could suggest reducing playback parameters, i.e., resolution, to reduce energy consumption during playback.

\subsection{Decode Energy Estimation}
\nameArchprof uses the hardware decoder profile to estimate decode energy for a specific video.
This process involves using the same instrumentation of {FFmpeg} described in Section~\ref{subsec:profiles}, but this time on a new video without power consumption details.
\nameArchprof uses {FFmpeg}'s trace of the new video's structure along with the profile's logs for previous videos to estimate decode energy for the new video.
Coupled with playback constraints, it can guide the encoding process to best optimize video decode energy consumption for a target architecture.


\subsection{Implementation \& Methodology}
\nameArchprof generated the results for our evaluation using hardware decoder profiles for each of the \nameArch configurations.
For \nameArch's custom scheduler hardware unit, we characterized performance and energy overhead from post-synthesis results from an RTL configuration.

\paragraph{Hardware Accelerator Design Details}
We constructed a baseline video decoder simulator from~\cite{hevcThesis} to emulate a scalable parallel design.
We implemented the \nameArch decode scheduler in RTL and synthesize the designs with Yosys~\cite{yosys}.
Our scheduler design targeted maximally supporting 128 parallel tiles.
% yosys output: 36942 area, in micron? so
Mapped to the Nangate FreePDK 45nm library, our scheduler incurred .035$mm^2$ and sustained a clock rate of 3.4 GHz, much more than required to maintain the 100 MHz peak clock rate of the baseline architecture.

\paragraph{Workloads}
\nameArchprof uses two video characterization workloads to evaluate decode characteristics: \texttt{vbench}, a representative benchmark dataset from YouTube~\cite{vbench}, and \texttt{vr-360}, a representative benchmark dataset of 4K-resolution, \threesixty video~\cite{vr360-mmsys17}.

\begin{table}[h]
  \centering
  \caption{Video Workloads Evaluated}
  \label{tab:workloads}

  \begin{tabular}{l|ll}
    \toprule
  Workload       & \texttt{vbench} {[}22{]} & \texttt{VR-360} {[}6{]}                   \\ \midrule
  Description    & YouTube         & 4K-\threesixty VR \\
  \# Videos      & 15              & 9                                \\
  Bitrate (Mbps) & 0.53-470        & 10-21                            \\
  Pixels / Frame & 410K - 8M       & 8M \\ \bottomrule
  \end{tabular}

\end{table}

We used the videos at their original encoded bitrate and constructed foveated tiled and multi-resolution patterns to encode them for VR consumption.
For both compression techniques, we assumed a fixed, center-weighted foveation pattern and generated videos accordingly.
The foveated tile formation was uniformly tiled, with one center tile with maximum quality and concentric rings of tiles around the center tile at 75\%, and 50\%.
We filled the rest of the frame with 10\% bitrate tiles.
The multi-resolution foveation pattern used two tiles: one for the entire video at 10\% bitrate, and another cropped to just the central foveal region at the original bitrate.
These tiles were generated and encoded by {FFmpeg}, invoked by \nameArchprof.
