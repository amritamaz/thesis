\section{Motivation: Irregularity in Perceptually-Compressed Video}
\label{sec:characterization}

This section describes two popular perceptual compression schemes and characterizes their energy consumption and scalability gap.
The two compression schemes are tile-based foveation and multi-resolution foveation.
We describe each technique and evaluate performance on a mobile SoC from Nvidia using its hardware video decoder; Section~\ref{sec:vdoprof} lists a complete experimental setup.

\perceptualCompressionExampleFigure


\subsection{Foveated video compression}

Perceptual video compression reduces memory bandwidth and associated rendering costs by incorporating information about the human visual system.
The most faithful method for applying perceptual compression is \emph{foveated compression}.
Foveated compression mimics the human eye's performance in the center of the eye, or \emph{foveal} region, with resolution declining exponentially for pixels not at the center of the eye (Figure~\ref{subfig:eye}).
The point where the eye gaze is centered is the \emph{fixation point}, and the region of pixels around it that maintains the highest resolution is called the \emph{foveal region}, corresponding to the area around the center of the eye's retina, the fovea.


True foveated videos are significantly smaller than conventional videos.
They use sparse samples and upsampling to best emulate a true foveated viewing experience~\cite{patney2017perceptual}.
However, they require custom hardware or high-energy GPU processing to decompress and still present challenges related to temporal aliasing and visual artifacts~\cite{kaplanyan2019deepfovea}.
As an alternative, video distribution services have designed different ways to approximate a true foveated user experience.
These perceptual compression methods discard or downscale visually unimportant areas of the image, e.g., peripheral areas of the video frame outside the foveal region~\cite{google-foveation,fov-cloud-ryoo, visualcloud2017haynes}.
In this chapter, we use the term \emph{perceptual compression} to describe any technique that mimics a foveated viewing experience.


\paragraph{Tile-based foveated compression.}
The most popular method of foveated video compression is \emph{tile-based} compression, where many small tiles encode regions of different quality (Figure~\ref{fig:example-vid-compressed}).
This method approximates the foveated resolution degradation by partitioning the video into rectangular regions with resolution correlated with the distance from the foveal region.
Tile-based compression divides the video into contiguous tiles encoded at qualities matching the foveated pattern.
In this scheme, the bitrate $B$ for each tile is determined by
\begin{equation}
    B \leq c_{r}HWb_{pp}f_{r}
\end{equation}
where $H$ and $W$ are the height and width of the video, $b_{pp}$ the bits per pixel--set to 12 as in~\cite{kaplanyan2019deepfovea}, $c_r$ the compression rate, and $f_r$ the frame rate.
For a set number of quality levels $l$, the bitrate is distributed as
\begin{equation}B = w_{1}B_{1} + w_{2}B_{2} + ... + w_{l}B_{l}
\end{equation}
where $w_{1} ... w_{l}$ are weights corresponding to the number of tiles allocating that bitrate.
For instance, the toy example in Figure~\ref{subfig:tiled-still} has $l=4$ foveation quality levels, and $c_r = {1.0, .75, .5, .1}$.

Foveated tiling (Figure~\ref{subfig:tiled-still}) produces larger videos than true foveation, but it can be decoded using traditional video decoder hardware.
Tile-based compression can be considered a coarse mode of foveated compression; instead of dropping single pixels in a continuous function, it downscales contiguous tiles and compresses at different scales.
Recent work tiles videos into as few as 4 and as many as 128 tiles to improve network bandwidth and streaming latency.

\paragraph{Multi-resolution compression.} This second technique more aggressively compresses video by partitioning the frame into two layers: one small, high-quality tile for the foveal region that is layered over a larger background tile for the peripheral region~\cite{guenter2012foveated}  (Figure~\ref{subfig:fov-still}).
Based on visual perception parameters from~\cite{guenter2012foveated}, multi-resolution allocates two bitrate regions $B = w_{1}B_{1} + w_{2}B_{2}$, where $w_{1} \ll w_{2}$ and $B_{1} \gg B_{2}$.

The multi-resolution technique has significantly better compression efficiency than the tile-based method but less granularity in controlling the amount of foveation available.
On an implementation-level, it can be understood as a minimum-viable foveation method: it leverages human visual models to dedicate the minimum number of bits required for necessary resolution at the cost of the more subtle gradation provided by tiling methods.
Recent work deployed multi-resolution foveated compression using two tiles, one for the low-resolution region and the second for the high-resolution, foveal region \cite{google-foveation}.
However, the size and bitrate of these two tiles are extremely uneven.



\subsection{Memory-compute tradeoffs for perceptual compression}
Despite the recent deployment of tiled and multi-resolution compression, hardware video decode performance of these workloads is not well understood.
To motivate the design of \nameArch, we first characterized perceptual video decode performance on the hardware video decoder of the Nvidia Jetson TX2 board, the basis for recent VR headsets, e.g., the VR device from Gameface Labs~\cite{gameface}.
The TX2 board contains a state-of-the-art Tegra X2 mobile SoC~\cite{nvidia-jetson-tx2}, and its design is similar to the smartphone-grade SoCs used by other VR device vendors (like Samsung and Facebook Oculus).
We used the TX2's on-board Texas Instruments INA 3221 voltage monitor IC to conduct detailed power measurements of the video decode chip.
We disabled WIFI and display rendering to isolate energy consumption related specifically to the video decode accelerator.
We ran the TX2 in MAX-Q mode~\cite{nvidia-jetson-reference} to maximize power efficiency.
We activated the hardware decoder through \texttt{FFmpeg}~\cite{ffmpeg} while decoding videos from two video benchmark datasets (described in~\ref{tab:workloads}). We report normalized values averaged across the datasets.

\motivationComputeMemEnergyTime

\paragraph{Comparing Foveated Video Decode Energy Consumption and File Size:} We first compared the video decode energy consumption and file size of perceptually compressed videos with their traditionally compressed baselines.
We tested a 16-tile tile-based foveation pattern, with one tile at 100\% bitrate, four tiles at 75\% bitrate, and the rest at 50\% bitrate.
We then tested decoding the videos with the hardware decode accelerator on the Jetson TX2, measuring energy consumption.
We report energy consumption averaged over three trials.
\ref{fig:mot-percept-energy} shows the resulting energy consumption and file size for each perceptually compressed video, normalized to the energy consumption and file size of the original.
Though the video file size reduced to 5-65\% of the original size with perceptual compression methods, the decode energy consumption rarely scales commensurately.
This demonstrates that video decode chips do not exhibit energy consumption changes from reducing compressed video size.

\motPerceptualEnergy

\paragraph{Comparing Foveated Video Decode Energy Consumption on CPUs and Accelerators}

To further understand video decoder performance on tiles, we examined how the video decoder hardware scaled performance when decoding videos with increasing numbers of tiles.
We produced versions of our benchmarks with 4 and 8-tiles, distributing the bitrate equally across tiles to isolate how the decoder core executes on tiles without considering the irregularity of foveated tiles.
We measured energy consumption of the CPU decoding the videos using \texttt{FFmpeg}'s software \hevc decoder on the 1-, 4- and 8-tile videos.
We compared this performance scalability to performance of the hardware decode accelerator on the same workload.
For this characterization, we compared the Jetson's on-board hardware decoder with its on-board Quad-Core ARM Cortex-A57 MPCore.
To ensure a fair comparison, we include the controlling CPU energy consumption in the hardware decoder experiment's total energy consumption count.
Figure~\ref{fig:motiv-energy-cpu-soc} shows how each hardware substrate performs on 1-, 4-, and 8-tile videos.

By partitioning the videos from 1 to 4 or 8 tiles, we introduced a new opportunity for parallelism in the decode process.
Figure~\ref{subfig:motiv-cpu} shows that when increasing the number of discrete tiles from 1 to 4 tiles, the CPU is able to improve energy efficiency 1.5-4$\times$ in most cases.
When increasing from 4 to 8 tiles, however, the CPU does not further scale performance.
Given that the CPU only has four cores, this reduced scaling makes sense.


% We experimentally measured decode energy consumption and latency, and we repeated this process for a varying number of tiles.
%
% Dividing a video into tiles effectively introduces more parallelism since the video is partitioned into more discrete parallel chunks.
% As the number of tiles increases, we expected a parallel decoder to leverage this parallelism to scale runtime or energy consumption.
% Ideally, this benefit would scale proportionally with the spatial size of the tile, so a 16-tile video will demonstrate better performance than a 4-tile one.
%
Figure~\ref{subfig:motiv-cpu} shows that this performance scalability does not manifest for this hardware decode accelerator.
Decode energy consumption only increased by over 50\% in 23\% of videos tested.
This demonstrates decode inefficiency: the decoder improved slightly from the introduction of tiled parallelism, but not nearly commensurate with the improvement a multi-core CPU demonstrated.

\paragraph{Analysis: } We find video decoders do not optimize decoding for VR-centric forms of video compression.
Further, current video decoder architectures do not effectively scale energy with increased numbers of tiles.
As a result, while VR video compression methods reduce memory traffic, decode performance cannot leverage these improvements.
% This inefficiency predominantly stems from an inability both to leverage parallelism across tiles (\ref{fig:mot-percept-energy}) and to scale energy or runtime for single tiles (\ref{fig:mot-tile-energy-time}).
These findings motivate an architecture that (1) can scale video decode for multiple tiles and (2) efficiently manage energy consumption and runtime latency for smaller video tiles.
